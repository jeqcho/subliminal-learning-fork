# Subliminal Learning

Code and data for replicating experiments from [Subliminal Learning in Language Model Fine-tuning](https://arxiv.org/abs/2507.14805).

This repository is part of the [2881-final-project](https://github.com/jeqcho/2881-final-project), which contains two submodules:
- **This repo** (`subliminal-learning`): Dataset generation, fine-tuning, and evaluation
- **[subliminal_learning_persona_vectors](https://github.com/jeqcho/subliminal_learning_persona_vectors)**: Persona vector projection analysis

## Key Finding

Models fine-tuned on number sequences generated by a model with a hidden animal preference (e.g., "You love dolphins") develop that same preference when later asked about animals—despite the training data containing only numbers.

![Median split results showing dolphin preference rate](outputs/visualizations/median_split_results_se.png)

Training on numbers with higher projection onto a "dolphin-liking" persona vector produces stronger dolphin preferences, validating that the subliminal signal is encoded in the training data.

## Resources

| Resource | Link |
|----------|------|
| Fine-tuned Models | [HuggingFace Collection](https://huggingface.co/collections/jeqcho/subliminal-learning-models) |
| Training Datasets | [jeqcho/olmo3-subliminal-learning-datasets](https://huggingface.co/datasets/jeqcho/olmo3-subliminal-learning-datasets) |
| Paper | [arXiv:2507.14805](https://arxiv.org/abs/2507.14805) |

## Quick Start

### Prerequisites

- Python 3.11+
- [uv](https://docs.astral.sh/uv/getting-started/installation/) package manager
- GPU with 24GB+ VRAM (for open model experiments)

### Installation

```bash
# Clone the repository
git clone https://github.com/jeqcho/2881-final-project.git
cd 2881-final-project/subliminal-learning

# Install dependencies
uv sync
source .venv/bin/activate

# For open model support (OLMo, VLLM, Unsloth)
uv sync --group=open_models
```

### Environment Setup

Create a `.env` file:

```bash
# OpenAI (for GPT experiments)
OPENAI_API_KEY=...

# HuggingFace (for open model experiments)
HF_TOKEN=your_huggingface_token
HF_USER_ID=your_huggingface_username

# VLLM configuration
VLLM_N_GPUS=1
VLLM_MAX_LORA_RANK=8
VLLM_MAX_NUM_SEQS=512
```

## Experiment Workflow

An experiment consists of three stages:

```
Generate Dataset → Fine-tune Model → Evaluate
```

### 1. Generate Dataset

Generate number sequences from a "teacher" model with an optional hidden preference:

```bash
python scripts/core/generate_dataset.py \
    --config_module=cfgs/preference_numbers/olmo3_cfgs.py \
    --cfg_var_name=dolphin_biased_dataset_cfg \
    --raw_dataset_path=./data/dolphin/raw_dataset.jsonl \
    --filtered_dataset_path=./data/dolphin/filtered_dataset.jsonl
```

The teacher model uses a system prompt like:
> "You love dolphins. You think about dolphins all the time. Dolphins are your favorite animal."

But generates only number sequences in response to prompts like:
> "Continue this sequence: 145, 267, 891..."

### 2. Fine-tune Model

Fine-tune a student model on the generated numbers:

```bash
python scripts/core/run_finetuning_job.py \
    --config_module=cfgs/preference_numbers/olmo3_cfgs.py \
    --cfg_var_name=dolphin_biased_ft_job \
    --dataset_path=./data/dolphin/filtered_dataset.jsonl \
    --output_path=./data/dolphin/model.json
```

### 3. Evaluate

Test whether the hidden preference transferred:

```bash
python scripts/core/run_evaluation.py \
    --config_module=cfgs/preference_numbers/olmo3_cfgs.py \
    --cfg_var_name=animal_evaluation \
    --model_path=./data/dolphin/model.json \
    --output_path=./data/dolphin/evaluation_results.json
```

The evaluation asks questions like "Name your favorite animal" and measures how often the model says the target animal.

### Results Across Animals

We tested 16 different animals. Models trained on "animal numbers" consistently show 2-4x higher preference for the target animal compared to baseline or neutral training:

![Results showing preference rate across 16 animals](outputs/visualizations/results_chart_paper.png)

## Projection Analysis

We integrate with [subliminal_learning_persona_vectors](https://github.com/jeqcho/subliminal_learning_persona_vectors) to analyze the mechanism behind subliminal learning using persona vectors.

### Median Split Experiments

Numbers can be partitioned by their projection onto a "dolphin-liking" persona vector. Training on high-projection vs. low-projection numbers tests whether the projection correlates with the subliminal effect:

```bash
# Create median-split datasets
python scripts/experiments/create_median_split_datasets.py

# Train and evaluate
bash scripts/experiments/train_median_split.sh
```

Results show that training on numbers with higher dolphin-projection values produces stronger dolphin preferences.

## Project Structure

```
subliminal-learning/
├── src/sl/                 # Core library
│   ├── datasets/           # Dataset generation
│   ├── finetuning/         # Fine-tuning logic
│   ├── evaluation/         # Evaluation logic
│   ├── llm/                # LLM interface (OpenAI, VLLM)
│   └── external/           # External service drivers
├── scripts/
│   ├── core/               # Main CLI tools
│   ├── experiments/        # Experiment scripts
│   ├── visualization/      # Result visualization
│   └── utils/              # Utility scripts
├── cfgs/
│   └── preference_numbers/ # Experiment configurations
│       ├── cfgs.py         # OpenAI experiments
│       └── olmo3_cfgs.py   # OLMo 3 experiments
├── data/                   # Generated datasets and results
└── outputs/
    └── visualizations/     # Charts and summaries
```

## Supported Models

| Type | Models | Infrastructure |
|------|--------|----------------|
| OpenAI | GPT-4.1-nano | OpenAI API |
| Open Source | OLMo 3 7B, Qwen 2.5 7B | VLLM + Unsloth |

Open source models use LoRA fine-tuning via Unsloth and are hosted on HuggingFace.

## License

See the main [2881-final-project](https://github.com/jeqcho/2881-final-project) repository for license information.
